import numpy as np
import pandas as pd
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import time

# GPU ì„¤ì •
print("=== GPU ì„¤ì • í™•ì¸ ===")
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    print(f"âœ… GPU ì„¤ì • ì™„ë£Œ: {len(gpus)}ê°œ")
else:
    print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# ëª¨ë¸ imports
from tensorflow.keras.applications import (
    DenseNet121, ResNet50, VGG16, MobileNet, EfficientNetB0
)
from tensorflow.keras.applications.imagenet_utils import preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense, GlobalAveragePooling2D, Dropout, BatchNormalization,
    Conv2D, MaxPooling2D, Flatten
)
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, 
    LearningRateScheduler, CSVLogger
)

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
os.chdir('C:/Users/ayaan/Documents/Git/Junior/Python/Data')
print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")

# ë°ì´í„° ê²½ë¡œ
train_dir = 'ani/train'
val_dir = 'ani/val'

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
os.makedirs('experiments', exist_ok=True)
os.makedirs('experiments/models', exist_ok=True)
os.makedirs('experiments/plots', exist_ok=True)
os.makedirs('experiments/logs', exist_ok=True)

class ExperimentLogger:
    def __init__(self):
        self.results = []
        
    def log_experiment(self, experiment_name, model_name, config, history, train_time):
        result = {
            'timestamp': datetime.now().isoformat(),
            'experiment_name': experiment_name,
            'model_name': model_name,
            'config': config,
            'train_time': train_time,
            'final_train_acc': float(history.history['accuracy'][-1]),
            'final_val_acc': float(history.history['val_accuracy'][-1]),
            'best_val_acc': float(max(history.history['val_accuracy'])),
            'final_train_loss': float(history.history['loss'][-1]),
            'final_val_loss': float(history.history['val_loss'][-1]),
            'epochs_trained': len(history.history['loss']),
            'overfitting_score': float(history.history['accuracy'][-1] - history.history['val_accuracy'][-1])
        }
        self.results.append(result)
        
        # ê°œë³„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        with open(f'experiments/logs/{experiment_name}_{model_name}.json', 'w') as f:
            json.dump(result, f, indent=2)
            
    def save_summary(self):
        df = pd.DataFrame(self.results)
        df.to_csv('experiments/experiment_summary.csv', index=False)
        return df

logger = ExperimentLogger()

# ë°ì´í„° ë¡œë” í•¨ìˆ˜ë“¤
def create_data_generators(augmentation_level='medium'):
    """ë‹¤ì–‘í•œ ê°•ë„ì˜ ë°ì´í„° ì¦ê°•"""
    
    base_gen = ImageDataGenerator(preprocessing_function=preprocess_input)
    
    if augmentation_level == 'none':
        train_gen = base_gen
    elif augmentation_level == 'light':
        train_gen = ImageDataGenerator(
            preprocessing_function=preprocess_input,
            horizontal_flip=True,
            rotation_range=10,
            width_shift_range=0.1,
            height_shift_range=0.1
        )
    elif augmentation_level == 'medium':
        train_gen = ImageDataGenerator(
            preprocessing_function=preprocess_input,
            horizontal_flip=True,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            zoom_range=0.15,
            shear_range=0.1
        )
    elif augmentation_level == 'heavy':
        train_gen = ImageDataGenerator(
            preprocessing_function=preprocess_input,
            horizontal_flip=True,
            rotation_range=30,
            width_shift_range=0.3,
            height_shift_range=0.3,
            zoom_range=0.3,
            shear_range=0.2,
            brightness_range=[0.7, 1.3],
            channel_shift_range=0.1
        )
    
    return train_gen, base_gen

def load_data(batch_size=32, augmentation='medium'):
    """ë°ì´í„° ë¡œë”©"""
    train_gen, val_gen = create_data_generators(augmentation)
    
    train_generator = train_gen.flow_from_directory(
        train_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=True
    )
    
    validation_generator = val_gen.flow_from_directory(
        val_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False
    )
    
    return train_generator, validation_generator

# ëª¨ë¸ ìƒì„± í•¨ìˆ˜ë“¤
def create_densenet_model(num_classes, regularization_config):
    """DenseNet121 ëª¨ë¸"""
    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    if regularization_config.get('freeze_base', True):
        base_model.trainable = False
    else:
        for layer in base_model.layers[:-regularization_config.get('unfreeze_layers', 30)]:
            layer.trainable = False
    
    model = Sequential([
        base_model,
        GlobalAveragePooling2D()
    ])
    
    # ì •ê·œí™” ë ˆì´ì–´ ì¶”ê°€
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_1']))
    
    # Dense ë ˆì´ì–´
    l1_reg = regularization_config.get('l1_reg', 0)
    l2_reg = regularization_config.get('l2_reg', 0)
    
    if l1_reg > 0 and l2_reg > 0:
        reg = l1_l2(l1=l1_reg, l2=l2_reg)
    elif l1_reg > 0:
        reg = l1(l1_reg)
    elif l2_reg > 0:
        reg = l2(l2_reg)
    else:
        reg = None
    
    model.add(Dense(regularization_config.get('dense_units', 1024), 
                   activation='relu', kernel_regularizer=reg))
    
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_2']))
    
    if regularization_config.get('second_dense', False):
        model.add(Dense(512, activation='relu', kernel_regularizer=reg))
        if regularization_config.get('dropout_3', 0) > 0:
            model.add(Dropout(regularization_config['dropout_3']))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

def create_resnet_model(num_classes, regularization_config):
    """ResNet50 ëª¨ë¸"""
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    if regularization_config.get('freeze_base', True):
        base_model.trainable = False
    else:
        for layer in base_model.layers[:-regularization_config.get('unfreeze_layers', 30)]:
            layer.trainable = False
    
    model = Sequential([
        base_model,
        GlobalAveragePooling2D()
    ])
    
    # ì •ê·œí™” ì ìš© (DenseNetê³¼ ë™ì¼í•œ ë¡œì§)
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_1']))
    
    l1_reg = regularization_config.get('l1_reg', 0)
    l2_reg = regularization_config.get('l2_reg', 0)
    
    if l1_reg > 0 and l2_reg > 0:
        reg = l1_l2(l1=l1_reg, l2=l2_reg)
    elif l1_reg > 0:
        reg = l1(l1_reg)
    elif l2_reg > 0:
        reg = l2(l2_reg)
    else:
        reg = None
    
    model.add(Dense(regularization_config.get('dense_units', 1024), 
                   activation='relu', kernel_regularizer=reg))
    
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_2']))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

def create_vgg_model(num_classes, regularization_config):
    """VGG16 ëª¨ë¸"""
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    if regularization_config.get('freeze_base', True):
        base_model.trainable = False
    
    model = Sequential([
        base_model,
        GlobalAveragePooling2D()
    ])
    
    # ì •ê·œí™” ì ìš©
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_1']))
    
    l1_reg = regularization_config.get('l1_reg', 0)
    l2_reg = regularization_config.get('l2_reg', 0)
    
    if l1_reg > 0 and l2_reg > 0:
        reg = l1_l2(l1=l1_reg, l2=l2_reg)
    elif l1_reg > 0:
        reg = l1(l1_reg)
    elif l2_reg > 0:
        reg = l2(l2_reg)
    else:
        reg = None
    
    model.add(Dense(regularization_config.get('dense_units', 1024), 
                   activation='relu', kernel_regularizer=reg))
    
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_2']))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

def create_mobilenet_model(num_classes, regularization_config):
    """MobileNet ëª¨ë¸"""
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    if regularization_config.get('freeze_base', True):
        base_model.trainable = False
    
    model = Sequential([
        base_model,
        GlobalAveragePooling2D()
    ])
    
    # ì •ê·œí™” ì ìš©
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_1']))
    
    l1_reg = regularization_config.get('l1_reg', 0)
    l2_reg = regularization_config.get('l2_reg', 0)
    
    if l1_reg > 0 and l2_reg > 0:
        reg = l1_l2(l1=l1_reg, l2=l2_reg)
    elif l1_reg > 0:
        reg = l1(l1_reg)
    elif l2_reg > 0:
        reg = l2(l2_reg)
    else:
        reg = None
    
    model.add(Dense(regularization_config.get('dense_units', 512), 
                   activation='relu', kernel_regularizer=reg))
    
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_2']))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

def create_custom_cnn_model(num_classes, regularization_config):
    """ì»¤ìŠ¤í…€ CNN ëª¨ë¸"""
    model = Sequential()
    
    # ì²« ë²ˆì§¸ Conv ë¸”ë¡
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    if regularization_config.get('dropout_conv1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_conv1']))
    
    # ë‘ ë²ˆì§¸ Conv ë¸”ë¡
    model.add(Conv2D(64, (3, 3), activation='relu'))
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    if regularization_config.get('dropout_conv2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_conv2']))
    
    # ì„¸ ë²ˆì§¸ Conv ë¸”ë¡
    model.add(Conv2D(128, (3, 3), activation='relu'))
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    if regularization_config.get('dropout_conv3', 0) > 0:
        model.add(Dropout(regularization_config['dropout_conv3']))
    
    # Dense ë ˆì´ì–´
    model.add(GlobalAveragePooling2D())
    
    l1_reg = regularization_config.get('l1_reg', 0)
    l2_reg = regularization_config.get('l2_reg', 0)
    
    if l1_reg > 0 and l2_reg > 0:
        reg = l1_l2(l1=l1_reg, l2=l2_reg)
    elif l1_reg > 0:
        reg = l1(l1_reg)
    elif l2_reg > 0:
        reg = l2(l2_reg)
    else:
        reg = None
    
    if regularization_config.get('dropout_1', 0) > 0:
        model.add(Dropout(regularization_config['dropout_1']))
    
    model.add(Dense(regularization_config.get('dense_units', 256), 
                   activation='relu', kernel_regularizer=reg))
    
    if regularization_config.get('batch_norm', False):
        model.add(BatchNormalization())
    
    if regularization_config.get('dropout_2', 0) > 0:
        model.add(Dropout(regularization_config['dropout_2']))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
def create_lr_scheduler(scheduler_type):
    """ë‹¤ì–‘í•œ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬"""
    if scheduler_type == 'step':
        def step_decay(epoch):
            initial_lrate = 0.001
            drop = 0.5
            epochs_drop = 10.0
            lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))
            return lrate
        return LearningRateScheduler(step_decay)
    
    elif scheduler_type == 'exponential':
        def exp_decay(epoch):
            initial_lrate = 0.001
            k = 0.1
            lrate = initial_lrate * np.exp(-k*epoch)
            return lrate
        return LearningRateScheduler(exp_decay)
    
    elif scheduler_type == 'cosine':
        def cosine_decay(epoch):
            initial_lrate = 0.001
            min_lrate = 0.00001
            epochs = 25
            lrate = min_lrate + (initial_lrate - min_lrate) * (1 + np.cos(np.pi * epoch / epochs)) / 2
            return lrate
        return LearningRateScheduler(cosine_decay)
    
    else:  # plateau
        return ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)

def run_experiment(experiment_name, model_func, model_name, config):
    """ì‹¤í—˜ ì‹¤í–‰"""
    print(f"\n{'='*50}")
    print(f"ì‹¤í—˜: {experiment_name}")
    print(f"ëª¨ë¸: {model_name}")
    print(f"ì„¤ì •: {config}")
    print('='*50)
    
    # ë°ì´í„° ë¡œë“œ
    train_gen, val_gen = load_data(
        batch_size=config.get('batch_size', 32),
        augmentation=config.get('augmentation', 'medium')
    )
    
    num_classes = train_gen.num_classes
    
    # ëª¨ë¸ ìƒì„±
    model = model_func(num_classes, config)
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer_name = config.get('optimizer', 'adam')
    lr = config.get('learning_rate', 0.001)
    
    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=lr)
    elif optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=lr, momentum=0.9)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=lr)
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
    checkpoint_path = f'experiments/models/{experiment_name}_{model_name}.h5'
    callbacks.append(ModelCheckpoint(
        checkpoint_path, 
        monitor='val_accuracy', 
        save_best_only=True, 
        mode='max'
    ))
    
    # ì¡°ê¸° ì¢…ë£Œ
    if config.get('early_stopping', True):
        callbacks.append(EarlyStopping(
            monitor='val_accuracy',
            patience=config.get('patience', 7),
            restore_best_weights=True
        ))
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    lr_scheduler = config.get('lr_scheduler', 'plateau')
    callbacks.append(create_lr_scheduler(lr_scheduler))
    
    # CSV ë¡œê±°
    csv_path = f'experiments/logs/{experiment_name}_{model_name}_log.csv'
    callbacks.append(CSVLogger(csv_path))
    
    # í›ˆë ¨
    start_time = time.time()
    
    history = model.fit(
        train_gen,
        epochs=config.get('epochs', 25),
        validation_data=val_gen,
        callbacks=callbacks,
        verbose=1,
        workers=1,
        use_multiprocessing=False
    )
    
    train_time = time.time() - start_time
    
    # ê²°ê³¼ ë¡œê¹…
    logger.log_experiment(experiment_name, model_name, config, history, train_time)
    
    print(f"âœ… ì‹¤í—˜ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {max(history.history['val_accuracy']):.4f}")
    
    return history, model

# ì‹¤í—˜ ì„¤ì •ë“¤
print("\nğŸš€ ì¢…í•© ì‹¤í—˜ ì‹œì‘!")
print("=" * 60)

# 1. ê¸°ë³¸ ëª¨ë¸ë“¤ ë¹„êµ (ê·œì œ ì—†ìŒ)
baseline_config = {
    'batch_size': 32,
    'epochs': 15,
    'learning_rate': 0.001,
    'optimizer': 'adam',
    'augmentation': 'medium',
    'freeze_base': True,
    'early_stopping': True,
    'patience': 5
}

models_to_test = [
    (create_densenet_model, 'DenseNet121'),
    (create_resnet_model, 'ResNet50'),
    (create_vgg_model, 'VGG16'),
    (create_mobilenet_model, 'MobileNet'),
    (create_custom_cnn_model, 'CustomCNN')
]

# ê¸°ë³¸ ëª¨ë¸ ë¹„êµ
print("\nğŸ“Š 1. ê¸°ë³¸ ëª¨ë¸ ë¹„êµ ì‹¤í—˜")
for model_func, model_name in models_to_test:
    try:
        run_experiment('baseline', model_func, model_name, baseline_config)
    except Exception as e:
        print(f"âŒ {model_name} ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        continue

# 2. ê·œì œ ê¸°ë²• ë¹„êµ (DenseNet121 ê¸°ì¤€)
print("\nğŸ“Š 2. ê·œì œ ê¸°ë²• ë¹„êµ ì‹¤í—˜")

regularization_configs = {
    'no_reg': baseline_config,
    
    'dropout_light': {**baseline_config, 'dropout_1': 0.3, 'dropout_2': 0.2},
    
    'dropout_heavy': {**baseline_config, 'dropout_1': 0.5, 'dropout_2': 0.3, 'dropout_3': 0.2, 'second_dense': True},
    
    'l2_reg': {**baseline_config, 'l2_reg': 0.001},
    
    'l1_reg': {**baseline_config, 'l1_reg': 0.001},
    
    'l1_l2_reg': {**baseline_config, 'l1_reg': 0.0005, 'l2_reg': 0.0005},
    
    'batch_norm': {**baseline_config, 'batch_norm': True},
    
    'combined_light': {**baseline_config, 'dropout_1': 0.3, 'dropout_2': 0.2, 'l2_reg': 0.0005, 'batch_norm': True},
    
    'combined_heavy': {**baseline_config, 'dropout_1': 0.5, 'dropout_2': 0.3, 'l1_reg': 0.0005, 'l2_reg': 0.0005, 'batch_norm': True, 'second_dense': True}
}

for reg_name, config in regularization_configs.items():
    try:
        run_experiment(f'regularization_{reg_name}', create_densenet_model, 'DenseNet121', config)
    except Exception as e:
        print(f"âŒ ê·œì œ ì‹¤í—˜ {reg_name} ì‹¤íŒ¨: {e}")
        continue

# 3. ë°ì´í„° ì¦ê°• ë¹„êµ
print("\nğŸ“Š 3. ë°ì´í„° ì¦ê°• ë¹„êµ ì‹¤í—˜")

augmentation_configs = {
    'no_aug': {**baseline_config, 'augmentation': 'none'},
    'light_aug': {**baseline_config, 'augmentation': 'light'},
    'medium_aug': {**baseline_config, 'augmentation': 'medium'},
    'heavy_aug': {**baseline_config, 'augmentation': 'heavy'}
}

for aug_name, config in augmentation_configs.items():
    try:
        run_experiment(f'augmentation_{aug_name}', create_densenet_model, 'DenseNet121', config)
    except Exception as e:
        print(f"âŒ ì¦ê°• ì‹¤í—˜ {aug_name} ì‹¤íŒ¨: {e}")
        continue

# 4. ì˜µí‹°ë§ˆì´ì € ë¹„êµ
print("\nğŸ“Š 4. ì˜µí‹°ë§ˆì´ì € ë¹„êµ ì‹¤í—˜")

optimizer_configs = {
    'adam': {**baseline_config, 'optimizer': 'adam'},
    'sgd': {**baseline_config, 'optimizer': 'sgd'},
    'rmsprop': {**baseline_config, 'optimizer': 'rmsprop'}
}

for opt_name, config in optimizer_configs.items():
    try:
        run_experiment(f'optimizer_{opt_name}', create_densenet_model, 'DenseNet121', config)
    except Exception as e:
        print(f"âŒ ì˜µí‹°ë§ˆì´ì € ì‹¤í—˜ {opt_name} ì‹¤íŒ¨: {e}")
        continue

# 5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„êµ
print("\nğŸ“Š 5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„êµ ì‹¤í—˜")

scheduler_configs = {
    'plateau': {**baseline_config, 'lr_scheduler': 'plateau'},
    'step': {**baseline_config, 'lr_scheduler': 'step'},
    'exponential': {**baseline_config, 'lr_scheduler': 'exponential'},
    'cosine': {**baseline_config, 'lr_scheduler': 'cosine'}
}

for sched_name, config in scheduler_configs.items():
    try:
        run_experiment(f'scheduler_{sched_name}', create_densenet_model, 'DenseNet121', config)
    except Exception as e:
        print(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í—˜ {sched_name} ì‹¤íŒ¨: {e}")
        continue

print("\nğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
print("=" * 60)

# ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
print("\nğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™” ìƒì„± ì¤‘...")

try:
    # ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
    results_df = logger.save_summary()
    
    # ìƒìœ„ ì‹¤í—˜ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ† Top 10 ì‹¤í—˜ ê²°ê³¼ (ê²€ì¦ ì •í™•ë„ ê¸°ì¤€):")
    top_results = results_df.nlargest(10, 'best_val_acc')[
        ['experiment_name', 'model_name', 'best_val_acc', 'overfitting_score', 'train_time']
    ]
    print(top_results.to_string(index=False))
    
    # ì‹œê°í™”
    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    
    # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    baseline_results = results_df[results_df['experiment_name'] == 'baseline']
    if len(baseline_results) > 0:
        axes[0, 0].bar(baseline_results['model_name'], baseline_results['best_val_acc'])
        axes[0, 0].set_title('ëª¨ë¸ë³„ ê²€ì¦ ì •í™•ë„ ë¹„êµ')
        axes[0, 0].set_ylabel('ê²€ì¦ ì •í™•ë„')
        axes[0, 0].tick_params(axis='x', rotation=45)
    
    # 2. ê·œì œ ê¸°ë²• íš¨ê³¼
    reg_results = results_df[results_df['experiment_name'].str.contains('regularization', na=False)]
    if len(reg_results) > 0:
        reg_names = reg_results['experiment_name'].str.replace('regularization_', '')
        axes[0, 1].bar(reg_names, reg_results['best_val_acc'])
        axes[0, 1].set_title('ê·œì œ ê¸°ë²•ë³„ ê²€ì¦ ì •í™•ë„')
        axes[0, 1].set_ylabel('ê²€ì¦ ì •í™•ë„')
        axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. ê³¼ì í•© ì ìˆ˜ ë¹„êµ
    axes[0, 2].scatter(results_df['best_val_acc'], results_df['overfitting_score'], alpha=0.7)
    axes[0, 2].set_xlabel('ê²€ì¦ ì •í™•ë„')
    axes[0, 2].set_ylabel('ê³¼ì í•© ì ìˆ˜ (í›ˆë ¨-ê²€ì¦ ì •í™•ë„ ì°¨ì´)')
    axes[0, 2].set_title('ê³¼ì í•© vs ì„±ëŠ¥ ë¶„ì„')
    axes[0, 2].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    # 4. ë°ì´í„° ì¦ê°• íš¨ê³¼
    aug_results = results_df[results_df['experiment_name'].str.contains('augmentation', na=False)]
    if len(aug_results) > 0:
        aug_names = aug_results['experiment_name'].str.replace('augmentation_', '')
        axes[1, 0].bar(aug_names, aug_results['best_val_acc'])
        axes[1, 0].set_title('ë°ì´í„° ì¦ê°•ë³„ ê²€ì¦ ì •í™•ë„')
        axes[1, 0].set_ylabel('ê²€ì¦ ì •í™•ë„')
        axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 5. í›ˆë ¨ ì‹œê°„ vs ì„±ëŠ¥
    axes[1, 1].scatter(results_df['train_time'], results_df['best_val_acc'], alpha=0.7)
    axes[1, 1].set_xlabel('í›ˆë ¨ ì‹œê°„ (ì´ˆ)')
    axes[1, 1].set_ylabel('ê²€ì¦ ì •í™•ë„')
    axes[1, 1].set_title('í›ˆë ¨ ì‹œê°„ vs ì„±ëŠ¥')
    
    # 6. ì˜µí‹°ë§ˆì´ì € ë¹„êµ
    opt_results = results_df[results_df['experiment_name'].str.contains('optimizer', na=False)]
    if len(opt_results) > 0:
        opt_names = opt_results['experiment_name'].str.replace('optimizer_', '')
        axes[1, 2].bar(opt_names, opt_results['best_val_acc'])
        axes[1, 2].set_title('ì˜µí‹°ë§ˆì´ì €ë³„ ê²€ì¦ ì •í™•ë„')
        axes[1, 2].set_ylabel('ê²€ì¦ ì •í™•ë„')
    
    plt.tight_layout()
    plt.savefig('experiments/plots/comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ìƒì„¸ ë¶„ì„ íˆíŠ¸ë§µ
    plt.figure(figsize=(15, 10))
    
    # ì‹¤í—˜ë³„ ì£¼ìš” ì§€í‘œ íˆíŠ¸ë§µ
    metrics_df = results_df.pivot_table(
        index='experiment_name', 
        columns='model_name', 
        values='best_val_acc', 
        fill_value=0
    )
    
    sns.heatmap(metrics_df, annot=True, fmt='.3f', cmap='viridis')
    plt.title('ì‹¤í—˜ë³„ ê²€ì¦ ì •í™•ë„ íˆíŠ¸ë§µ')
    plt.tight_layout()
    plt.savefig('experiments/plots/accuracy_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê³¼ì í•© ë¶„ì„
    plt.figure(figsize=(12, 8))
    
    # ê³¼ì í•© ì •ë„ë³„ ìƒ‰ìƒ êµ¬ë¶„
    colors = ['green' if x < 0.05 else 'yellow' if x < 0.1 else 'red' for x in results_df['overfitting_score']]
    
    plt.scatter(results_df['best_val_acc'], results_df['overfitting_score'], 
               c=colors, alpha=0.7, s=100)
    
    # ê° ì ì— ì‹¤í—˜ëª… ë¼ë²¨
    for i, row in results_df.iterrows():
        plt.annotate(f"{row['experiment_name'][:10]}", 
                    (row['best_val_acc'], row['overfitting_score']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.xlabel('ê²€ì¦ ì •í™•ë„')
    plt.ylabel('ê³¼ì í•© ì ìˆ˜')
    plt.title('ê³¼ì í•© ë¶„ì„ (ë…¹ìƒ‰: ì¢‹ìŒ, ë…¸ë‘: ë³´í†µ, ë¹¨ê°•: ê³¼ì í•©)')
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    plt.axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='ê³¼ì í•© ê²½ê³„ (0.05)')
    plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='ì‹¬ê°í•œ ê³¼ì í•© (0.1)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('experiments/plots/overfitting_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nğŸ“Š ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    report = f"""
    ================================================================================
    ğŸ”¬ ì¢…í•© ì‹¤í—˜ ë¶„ì„ ë¦¬í¬íŠ¸
    ================================================================================
    
    ğŸ“… ì‹¤í—˜ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    ğŸ”¢ ì´ ì‹¤í—˜ ìˆ˜: {len(results_df)}
    
    ğŸ† ìµœê³  ì„±ëŠ¥ ì‹¤í—˜:
    {'-'*50}
    """
    
    best_experiment = results_df.loc[results_df['best_val_acc'].idxmax()]
    report += f"""
    ì‹¤í—˜ëª…: {best_experiment['experiment_name']}
    ëª¨ë¸: {best_experiment['model_name']}
    ê²€ì¦ ì •í™•ë„: {best_experiment['best_val_acc']:.4f}
    ê³¼ì í•© ì ìˆ˜: {best_experiment['overfitting_score']:.4f}
    í›ˆë ¨ ì‹œê°„: {best_experiment['train_time']:.2f}ì´ˆ
    """
    
    # ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥
    report += f"\n\nğŸ“ˆ ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥:\n{'-'*50}\n"
    model_best = results_df.groupby('model_name')['best_val_acc'].max().sort_values(ascending=False)
    for model, acc in model_best.items():
        report += f"{model}: {acc:.4f}\n"
    
    # ê·œì œ ê¸°ë²•ë³„ íš¨ê³¼
    reg_results = results_df[results_df['experiment_name'].str.contains('regularization', na=False)]
    if len(reg_results) > 0:
        report += f"\n\nğŸ›¡ï¸ ê·œì œ ê¸°ë²•ë³„ íš¨ê³¼:\n{'-'*50}\n"
        reg_best = reg_results.groupby('experiment_name')['best_val_acc'].max().sort_values(ascending=False)
        for exp, acc in reg_best.items():
            reg_name = exp.replace('regularization_', '')
            report += f"{reg_name}: {acc:.4f}\n"
    
    # ê³¼ì í•© ë¶„ì„
    low_overfitting = results_df[results_df['overfitting_score'] < 0.05]
    medium_overfitting = results_df[(results_df['overfitting_score'] >= 0.05) & (results_df['overfitting_score'] < 0.1)]
    high_overfitting = results_df[results_df['overfitting_score'] >= 0.1]
    
    report += f"""
    
    ğŸ¯ ê³¼ì í•© ë¶„ì„:
    {'-'*50}
    ê³¼ì í•© ì—†ìŒ (< 0.05): {len(low_overfitting)}ê°œ ì‹¤í—˜
    ê²½ë¯¸í•œ ê³¼ì í•© (0.05-0.1): {len(medium_overfitting)}ê°œ ì‹¤í—˜
    ì‹¬ê°í•œ ê³¼ì í•© (> 0.1): {len(high_overfitting)}ê°œ ì‹¤í—˜
    
    """
    
    if len(low_overfitting) > 0:
        best_balanced = low_overfitting.loc[low_overfitting['best_val_acc'].idxmax()]
        report += f"""
    ğŸ¯ ìµœì  ê· í˜• ì‹¤í—˜ (ê³¼ì í•© ì—†ìœ¼ë©´ì„œ ê³ ì„±ëŠ¥):
    ì‹¤í—˜ëª…: {best_balanced['experiment_name']}
    ëª¨ë¸: {best_balanced['model_name']}
    ê²€ì¦ ì •í™•ë„: {best_balanced['best_val_acc']:.4f}
    ê³¼ì í•© ì ìˆ˜: {best_balanced['overfitting_score']:.4f}
    """
    
    # íš¨ìœ¨ì„± ë¶„ì„ (ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥)
    results_df['efficiency'] = results_df['best_val_acc'] / (results_df['train_time'] / 60)  # ë¶„ë‹¹ ì •í™•ë„
    most_efficient = results_df.loc[results_df['efficiency'].idxmax()]
    
    report += f"""
    
    âš¡ íš¨ìœ¨ì„± ë¶„ì„ (ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥):
    {'-'*50}
    ê°€ì¥ íš¨ìœ¨ì ì¸ ì‹¤í—˜:
    ì‹¤í—˜ëª…: {most_efficient['experiment_name']}
    ëª¨ë¸: {most_efficient['model_name']}
    ê²€ì¦ ì •í™•ë„: {most_efficient['best_val_acc']:.4f}
    í›ˆë ¨ ì‹œê°„: {most_efficient['train_time']:.2f}ì´ˆ
    íš¨ìœ¨ì„± ì ìˆ˜: {most_efficient['efficiency']:.4f}
    """
    
    # ê¶Œì¥ ì‚¬í•­
    report += f"""
    
    ğŸ’¡ ê¶Œì¥ ì‚¬í•­:
    {'-'*50}
    1. ìµœê³  ì„±ëŠ¥: {best_experiment['experiment_name']} ({best_experiment['model_name']})
    2. ìµœì  ê· í˜•: {best_balanced['experiment_name'] if len(low_overfitting) > 0 else 'ê³¼ì í•© ì—†ëŠ” ì‹¤í—˜ ì—†ìŒ'}
    3. ìµœê³  íš¨ìœ¨: {most_efficient['experiment_name']} ({most_efficient['model_name']})
    
    ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ íŒ:
    - {'ê·œì œ ê¸°ë²• ì¶”ê°€ í•„ìš”' if len(high_overfitting) > len(low_overfitting) else 'í˜„ì¬ ê·œì œ ìˆ˜ì¤€ ì ì ˆ'}
    - {'ë°ì´í„° ì¦ê°• ê°•í™” ê³ ë ¤' if results_df['best_val_acc'].max() < 0.9 else 'í˜„ì¬ ì„±ëŠ¥ ë§Œì¡±ìŠ¤ëŸ¬ì›€'}
    - {'ë” ê¸´ í›ˆë ¨ ì‹œê°„ ê³ ë ¤' if results_df['epochs_trained'].mean() < 20 else 'ì ì ˆí•œ í›ˆë ¨ ì‹œê°„'}
    
    ================================================================================
    """
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open('experiments/comprehensive_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    
    # ìµœì¢… ì¶”ì²œ ëª¨ë¸ ì„¤ì •
    print("\nğŸ¯ ìµœì¢… ì¶”ì²œ ì„¤ì •:")
    print("="*60)
    
    if len(low_overfitting) > 0:
        recommended = best_balanced
    else:
        recommended = best_experiment
    
    print(f"ì¶”ì²œ ì‹¤í—˜: {recommended['experiment_name']}")
    print(f"ì¶”ì²œ ëª¨ë¸: {recommended['model_name']}")
    print(f"ì˜ˆìƒ ì„±ëŠ¥: {recommended['best_val_acc']:.4f}")
    print(f"ê³¼ì í•© ìœ„í—˜: {'ë‚®ìŒ' if recommended['overfitting_score'] < 0.05 else 'ë³´í†µ' if recommended['overfitting_score'] < 0.1 else 'ë†’ìŒ'}")
    
    # ì¶”ì²œ ì„¤ì • ì €ì¥
    recommended_config = {
        'experiment_name': recommended['experiment_name'],
        'model_name': recommended['model_name'],
        'config': recommended['config'],
        'expected_accuracy': float(recommended['best_val_acc']),
        'overfitting_risk': 'low' if recommended['overfitting_score'] < 0.05 else 'medium' if recommended['overfitting_score'] < 0.1 else 'high'
    }
    
    with open('experiments/recommended_config.json', 'w') as f:
        json.dump(recommended_config, f, indent=2)
    
    print(f"\nğŸ’¾ ëª¨ë“  ê²°ê³¼ê°€ 'experiments' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("- experiment_summary.csv: ì „ì²´ ì‹¤í—˜ ê²°ê³¼")
    print("- comprehensive_analysis_report.txt: ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸") 
    print("- recommended_config.json: ì¶”ì²œ ì„¤ì •")
    print("- plots/: ëª¨ë“  ì‹œê°í™” ê²°ê³¼")
    print("- models/: í›ˆë ¨ëœ ëª¨ë¸ë“¤")
    print("- logs/: í›ˆë ¨ ë¡œê·¸ë“¤")

except Exception as e:
    print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()

print("\nğŸ‰ ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
print("="*60)
print("RTX 3060ì„ í™œìš©í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ìµœì í™” ì‹¤í—˜ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print("ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")