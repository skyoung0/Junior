import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import re
from pathlib import Path

class ClimateTechScraper:
    def __init__(self):
        self.base_url = 'https://www.ctis.re.kr/ko/techClass/classification.do?key=1141'
        self.output_dir = Path('assets/data/scraped')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸°
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # ChromeDriverManagerë¡œ ìë™ ë‹¤ìš´ë¡œë“œ
        service = webdriver.chrome.service.Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver
        
    def scrape_classification_basic(self):
        """ê¸°í›„ê¸°ìˆ  ê¸°ë³¸ ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§ (R ì½”ë“œ Python ë³€í™˜)"""
        print("ğŸ” ê¸°í›„ê¸°ìˆ  ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            response = requests.get(self.base_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì†Œë¶„ë¥˜(L3) ìˆ˜ì§‘
            l3_elements = soup.select('#table_box > table > tbody > tr > td.bgw')
            l3_data = []
            
            for i, element in enumerate(l3_elements, 1):
                l3_text = element.get_text(strip=True)
                if l3_text:
                    l3_data.append({
                        'No': i,
                        'L3': l3_text
                    })
            
            # ì¤‘ë¶„ë¥˜ ìœ„ì¹˜ (R ì½”ë“œì˜ L2_List ì°¸ê³ )
            l2_positions = [1, 4, 12, 14, 16, 18, 21, 23, 27, 31, 33, 36, 38, 41]
            
            # ì¤‘ë¶„ë¥˜(L2) ìˆ˜ì§‘
            l2_data = {}
            rows = soup.select('#table_box > table > tbody > tr')
            
            for pos in l2_positions:
                if pos <= len(rows):
                    row = rows[pos - 1]  # 0-based index
                    
                    # ë‹¤ì–‘í•œ ì—´ì—ì„œ ì¤‘ë¶„ë¥˜ í…ìŠ¤íŠ¸ ì°¾ê¸°
                    cells = row.find_all('td')
                    l2_text = None
                    
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        if text and len(text) > 3:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            l2_text = text
                            break
                    
                    if l2_text:
                        l2_data[pos] = self.clean_text(l2_text)
            
            # ëŒ€ë¶„ë¥˜ ìœ„ì¹˜
            l1_positions = [1, 23, 41]
            l1_data = {}
            
            for pos in l1_positions:
                if pos <= len(rows):
                    row = rows[pos - 1]
                    cells = row.find_all('td')
                    
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        if any(keyword in text for keyword in ['ê°ì¶•', 'ì ì‘', 'ìœµë³µí•©']):
                            l1_data[pos] = self.clean_text(text)
                            break
            
            # ë°ì´í„° ë³‘í•©
            result_data = []
            current_l1 = ""
            current_l2 = ""
            
            for item in l3_data:
                no = item['No']
                l3 = item['L3']
                
                # L1 ì—…ë°ì´íŠ¸
                for l1_pos in l1_positions:
                    if no >= l1_pos and l1_pos in l1_data:
                        current_l1 = l1_data[l1_pos]
                
                # L2 ì—…ë°ì´íŠ¸
                for l2_pos in l2_positions:
                    if no >= l2_pos and l2_pos in l2_data:
                        current_l2 = l2_data[l2_pos]
                
                result_data.append({
                    'L1_ëŒ€ë¶„ë¥˜': current_l1,
                    'L2_ì¤‘ë¶„ë¥˜': current_l2,
                    'L3_ì†Œë¶„ë¥˜': l3,
                    'No': no
                })
            
            # DataFrame ìƒì„± ë° ì €ì¥
            df = pd.DataFrame(result_data)
            output_file = self.output_dir / 'climate_tech_classification.csv'
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"âœ… ê¸°ë³¸ ë¶„ë¥˜ì²´ê³„ ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {len(df)}ê°œ í•­ëª©")
            
            return df
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def scrape_detailed_info(self):
        """ê¸°í›„ê¸°ìˆ  ìƒì„¸ì •ë³´ í¬ë¡¤ë§ (Python ì½”ë“œ ê°œì„ )"""
        print("ğŸ” ê¸°í›„ê¸°ìˆ  ìƒì„¸ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        driver = None
        try:
            driver = self.setup_driver()
            driver.get(self.base_url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, "table_box"))
            )
            
            detailed_data = []
            
            # ê° ë¶„ë¥˜ë³„ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
            categories = ['ê°ì¶•', 'ì ì‘', 'ìœµë³µí•©']
            
            for category in categories:
                print(f"ğŸ“‹ {category} ê¸°ìˆ  ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                
                try:
                    # ì¹´í…Œê³ ë¦¬ë³„ ë§í¬ í´ë¦­
                    if category == 'ê°ì¶•':
                        driver.find_element(By.XPATH, '//*[@id="tc1_anchor"]').click()
                    elif category == 'ì ì‘':
                        driver.find_element(By.XPATH, '//*[@id="tc2_anchor"]').click()
                    elif category == 'ìœµë³µí•©':
                        driver.find_element(By.XPATH, '//*[@id="tc3_anchor"]').click()
                    
                    time.sleep(2)
                    
                    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ê¸°ìˆ  í•­ëª© ìˆ˜ì§‘
                    tech_links = driver.find_elements(By.CSS_SELECTOR, f"#{category.lower()}_tech_list a")
                    
                    for i, link in enumerate(tech_links):
                        try:
                            link.click()
                            time.sleep(1)
                            
                            # ìƒì„¸ì •ë³´ ì¶”ì¶œ
                            detail_info = self.extract_detail_info(driver, category)
                            if detail_info:
                                detailed_data.append(detail_info)
                            
                            if i % 5 == 0:  # ì§„í–‰ìƒí™© ì¶œë ¥
                                print(f"   ì§„í–‰: {i+1}/{len(tech_links)}")
                                
                        except Exception as e:
                            print(f"   âš ï¸ ê°œë³„ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                            continue
                    
                except Exception as e:
                    print(f"   âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ê²°ê³¼ ì €ì¥
            if detailed_data:
                df = pd.DataFrame(detailed_data)
                output_file = self.output_dir / 'climate_tech_detailed.csv'
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                
                print(f"âœ… ìƒì„¸ì •ë³´ ì €ì¥ ì™„ë£Œ: {output_file}")
                print(f"ğŸ“Š ìˆ˜ì§‘ëœ ìƒì„¸ì •ë³´: {len(df)}ê°œ í•­ëª©")
                
                return df
            else:
                print("âŒ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            print(f"âŒ ìƒì„¸ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return None
        finally:
            if driver:
                driver.quit()
    
    def extract_detail_info(self, driver, category):
        """ìƒì„¸ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            subtitle = driver.find_element(By.CSS_SELECTOR, ".tech-title").text.strip()
            
            # ìƒì„¸ì •ë³´ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
            info_data = {
                'category': category,
                'subtitle': subtitle,
                'definition': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[1]/table/tbody/tr/td[1]'),
                'keywords_kor': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[2]/table/tbody/tr[1]/td'),
                'keywords_eng': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[2]/table/tbody/tr[2]/td'),
                'leading_country': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[3]/table/tbody/tr[1]/td'),
                'tech_level_pct': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[3]/table/tbody/tr[2]/td'),
                'tech_gap': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[3]/table/tbody/tr[3]/td'),
                'classification': self.safe_get_text(driver, '//*[@id="pdfShows"]/dl/dd[4]/table/tbody/tr/td')
            }
            
            return info_data
            
        except Exception as e:
            print(f"   âš ï¸ ìƒì„¸ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def safe_get_text(self, driver, xpath):
        """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            element = driver.find_element(By.XPATH, xpath)
            return self.clean_text(element.text)
        except:
            return ""
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # ê°œí–‰ë¬¸ì, íƒ­, ìºë¦¬ì§€ë¦¬í„´ ì œê±°
        text = re.sub(r'[\r\n\t]+', ' ', text)
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def create_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš©)"""
        print("ğŸ“‹ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ ë¶„ë¥˜ì²´ê³„ ìƒ˜í”Œ
        classification_sample = [
            {'L1_ëŒ€ë¶„ë¥˜': 'ê°ì¶•', 'L2_ì¤‘ë¶„ë¥˜': 'ì¬ìƒì—ë„ˆì§€', 'L3_ì†Œë¶„ë¥˜': 'íƒœì–‘ê´‘ ë°œì „', 'No': 1},
            {'L1_ëŒ€ë¶„ë¥˜': 'ê°ì¶•', 'L2_ì¤‘ë¶„ë¥˜': 'ì¬ìƒì—ë„ˆì§€', 'L3_ì†Œë¶„ë¥˜': 'í’ë ¥ ë°œì „', 'No': 2},
            {'L1_ëŒ€ë¶„ë¥˜': 'ê°ì¶•', 'L2_ì¤‘ë¶„ë¥˜': 'ì¬ìƒì—ë„ˆì§€', 'L3_ì†Œë¶„ë¥˜': 'ìˆ˜ë ¥ ë°œì „', 'No': 3},
            {'L1_ëŒ€ë¶„ë¥˜': 'ê°ì¶•', 'L2_ì¤‘ë¶„ë¥˜': 'ë¹„ì¬ìƒì—ë„ˆì§€', 'L3_ì†Œë¶„ë¥˜': 'ì›ìë ¥ ë°œì „', 'No': 4},
            {'L1_ëŒ€ë¶„ë¥˜': 'ê°ì¶•', 'L2_ì¤‘ë¶„ë¥˜': 'ì—ë„ˆì§€ì €ì¥', 'L3_ì†Œë¶„ë¥˜': 'ë°°í„°ë¦¬ ì €ì¥', 'No': 5},
            {'L1_ëŒ€ë¶„ë¥˜': 'ì ì‘', 'L2_ì¤‘ë¶„ë¥˜': 'ë¬¼ê´€ë¦¬', 'L3_ì†Œë¶„ë¥˜': 'í™ìˆ˜ ë°©ì–´', 'No': 6},
            {'L1_ëŒ€ë¶„ë¥˜': 'ì ì‘', 'L2_ì¤‘ë¶„ë¥˜': 'ë†ì—…', 'L3_ì†Œë¶„ë¥˜': 'ìŠ¤ë§ˆíŠ¸íŒœ', 'No': 7},
            {'L1_ëŒ€ë¶„ë¥˜': 'ìœµë³µí•©', 'L2_ì¤‘ë¶„ë¥˜': 'ICT ìœµí•©', 'L3_ì†Œë¶„ë¥˜': 'ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ', 'No': 8}
        ]
        
        df_classification = pd.DataFrame(classification_sample)
        output_file = self.output_dir / 'climate_tech_classification.csv'
        df_classification.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # ìƒì„¸ì •ë³´ ìƒ˜í”Œ
        detailed_sample = [
            {
                'category': 'ê°ì¶•',
                'subtitle': 'íƒœì–‘ê´‘ ë°œì „',
                'definition': 'íƒœì–‘ê´‘ì„ ì´ìš©í•˜ì—¬ ì „ê¸°ë¥¼ ìƒì‚°í•˜ëŠ” ê¸°ìˆ ',
                'keywords_kor': 'íƒœì–‘ê´‘, íƒœì–‘ì „ì§€, ì‹¤ë¦¬ì½˜',
                'keywords_eng': 'Solar, Photovoltaic, Silicon',
                'leading_country': 'ì¤‘êµ­',
                'tech_level_pct': '85%',
                'tech_gap': '2-3ë…„',
                'classification': 'ì‹ ì¬ìƒì—ë„ˆì§€ > íƒœì–‘ê´‘'
            },
            {
                'category': 'ê°ì¶•',
                'subtitle': 'í’ë ¥ ë°œì „',
                'definition': 'ë°”ëŒì˜ ìš´ë™ì—ë„ˆì§€ë¥¼ ì „ê¸°ì—ë„ˆì§€ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ',
                'keywords_kor': 'í’ë ¥, í„°ë¹ˆ, ë°œì „ê¸°',
                'keywords_eng': 'Wind, Turbine, Generator',
                'leading_country': 'ë´ë§ˆí¬',
                'tech_level_pct': '80%',
                'tech_gap': '3-5ë…„',
                'classification': 'ì‹ ì¬ìƒì—ë„ˆì§€ > í’ë ¥'
            }
        ]
        
        df_detailed = pd.DataFrame(detailed_sample)
        output_file = self.output_dir / 'climate_tech_detailed.csv'
        df_detailed.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print("âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        return df_classification, df_detailed

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = ClimateTechScraper()
    
    print("ğŸš€ ê¸°í›„ê¸°ìˆ  ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    # ê¸°ë³¸ ë¶„ë¥˜ì²´ê³„ í¬ë¡¤ë§
    classification_df = scraper.scrape_classification_basic()
    
    # ìƒì„¸ì •ë³´ í¬ë¡¤ë§
    detailed_df = scraper.scrape_detailed_info()
    
    # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    if classification_df is None or detailed_df is None:
        print("âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        scraper.create_sample_data()
    
    print("=" * 50)
    print("ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    main()