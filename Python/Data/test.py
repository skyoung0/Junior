import numpy as np
import pandas as pd
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import time
from sklearn.utils.class_weight import compute_class_weight

# TensorFlow λ²„μ „ ν™•μΈ λ° νΈν™μ„± μ„¤μ •
print(f"TensorFlow λ²„μ „: {tf.__version__}")

# GPU μµμ ν™” μ„¤μ •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("β… GPU μ„¤μ • μ™„λ£")
    except RuntimeError as e:
        print(f"GPU μ„¤μ • μ¤λ¥: {e}")

# νΈν™μ„±μ„ μ„ν• import
from tensorflow.keras.applications import EfficientNetB0, MobileNetV2, DenseNet121
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input,
    Conv2D, SeparableConv2D, Activation, Multiply, GlobalMaxPooling2D, 
    Concatenate, Add, LayerNormalization, MultiHeadAttention
)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical

# μµν‹°λ§μ΄μ € νΈν™μ„± μ²λ¦¬
try:
    from tensorflow.keras.optimizers import AdamW
    ADAMW_AVAILABLE = True
    print("β… AdamW μ‚¬μ© κ°€λ¥")
except ImportError:
    try:
        import tensorflow_addons as tfa
        AdamW = tfa.optimizers.AdamW
        ADAMW_AVAILABLE = True
        print("β… TensorFlow Addons AdamW μ‚¬μ©")
    except ImportError:
        from tensorflow.keras.optimizers import Adam
        ADAMW_AVAILABLE = False
        print("β οΈ AdamW μ—†μ, Adam μ‚¬μ©")

# κ²½λ΅ μ„¤μ •
os.chdir('C:/Users/ayaan/Documents/Git/Junior/Python/Data')
train_dir = 'ani/train'
val_dir = 'ani/val'

print("π€ νΈν™μ„± κ°μ„ λ μ• λ‹λ©”μ΄μ… κ°μ • μΈμ‹ λ¨λΈ")
print("=" * 60)

# π― κ°•ν™”λ λ°μ΄ν„° μ¦κ°•
def create_advanced_augmentation():
    return ImageDataGenerator(
        rescale=1./255,
        horizontal_flip=True,
        rotation_range=30,
        width_shift_range=0.3,
        height_shift_range=0.3,
        zoom_range=0.3,
        shear_range=0.2,
        brightness_range=[0.7, 1.3],
        channel_shift_range=20,
        fill_mode='nearest'
    )

def create_validation_generator():
    return ImageDataGenerator(rescale=1./255)

# π“ λ°μ΄ν„° λ΅λ”©
def load_data(image_size, batch_size):
    train_datagen = create_advanced_augmentation()
    val_datagen = create_validation_generator()
    
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(image_size, image_size),
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=True
    )
    
    val_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=(image_size, image_size),
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False
    )
    
    return train_generator, val_generator

# π—οΈ ν•μ΄λΈλ¦¬λ“ λ¨λΈ (νΈν™μ„± κ°μ„ )
def create_hybrid_attention_model(num_classes=7):
    """Attention λ©”μ»¤λ‹μ¦μ΄ ν¬ν•¨λ ν•μ΄λΈλ¦¬λ“ λ¨λΈ"""
    inputs = Input(shape=(224, 224, 3))
    
    # EfficientNetB0 λ°±λ³Έ
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_tensor=inputs
    )
    
    # Fine-tuning μ„¤μ •
    base_model.trainable = True
    total_layers = len(base_model.layers)
    freeze_until = int(total_layers * 0.8)
    
    for i, layer in enumerate(base_model.layers):
        layer.trainable = i >= freeze_until
    
    x = base_model.output
    
    # Global Averageμ™€ Max Pooling κ²°ν•©
    gap = GlobalAveragePooling2D()(x)
    gmp = GlobalMaxPooling2D()(x)
    
    # Attention λ©”μ»¤λ‹μ¦ (κ°„λ‹¨ λ²„μ „)
    attention_weights = Dense(gap.shape[-1], activation='sigmoid', name='attention')(gap)
    attended_features = Multiply()([gap, attention_weights])
    
    # Feature μµν•©
    combined_features = Concatenate()([attended_features, gmp])
    
    # λ¶„λ¥ ν—¤λ“
    x = BatchNormalization()(combined_features)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=predictions)
    return model

def create_enhanced_efficientnet_model(num_classes=7):
    """κ°•ν™”λ EfficientNet λ¨λΈ"""
    inputs = Input(shape=(224, 224, 3))
    
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_tensor=inputs
    )
    
    # Fine-tuning
    base_model.trainable = True
    total_layers = len(base_model.layers)
    freeze_until = int(total_layers * 0.8)
    
    for i, layer in enumerate(base_model.layers):
        layer.trainable = i >= freeze_until
    
    x = base_model.output
    
    # SE (Squeeze-and-Excitation) λΈ”λ΅
    se = GlobalAveragePooling2D()(x)
    se = Dense(se.shape[-1] // 16, activation='relu')(se)
    se = Dense(x.shape[-1], activation='sigmoid')(se)
    
    # Reshape for multiplication
    se = tf.keras.layers.Reshape((1, 1, x.shape[-1]))(se)
    x = Multiply()([x, se])
    
    # Global pooling
    x = GlobalAveragePooling2D()(x)
    
    # λ¶„λ¥ ν—¤λ“
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='swish', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='swish', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=predictions)
    return model

def create_densenet_ensemble_model(num_classes=7):
    """DenseNet κΈ°λ° μ•™μƒλΈ” λ¨λΈ"""
    inputs = Input(shape=(224, 224, 3))
    
    base_model = DenseNet121(
        weights='imagenet',
        include_top=False,
        input_tensor=inputs
    )
    
    # Fine-tuning
    base_model.trainable = True
    total_layers = len(base_model.layers)
    freeze_until = int(total_layers * 0.8)
    
    for i, layer in enumerate(base_model.layers):
        layer.trainable = i >= freeze_until
    
    x = base_model.output
    
    # Multi-scale feature extraction
    x = GlobalAveragePooling2D()(x)
    
    # λ¶„λ¥ ν—¤λ“
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.2)(x)
    
    predictions = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=predictions)
    return model

# π€ νΈν™ κ°€λ¥ν• μ»΄νμΌ ν•¨μ
def compile_model_compatible(model, learning_rate=1e-4):
    """νΈν™μ„±μ„ κ³ λ ¤ν• λ¨λΈ μ»΄νμΌ"""
    
    if ADAMW_AVAILABLE:
        if hasattr(AdamW, '__module__') and 'addons' in AdamW.__module__:
            # TensorFlow Addonsμ AdamW
            optimizer = AdamW(
                learning_rate=learning_rate,
                weight_decay=1e-5
            )
        else:
            # Kerasμ AdamW
            optimizer = AdamW(
                learning_rate=learning_rate,
                weight_decay=1e-5
            )
        print(f"β… AdamW μµν‹°λ§μ΄μ € μ‚¬μ© (lr={learning_rate})")
    else:
        optimizer = Adam(learning_rate=learning_rate)
        print(f"β… Adam μµν‹°λ§μ΄μ € μ‚¬μ© (lr={learning_rate})")
    
    # Focal Loss λ€μ‹  Label Smoothing μ μ©
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# π― μ½λ°± ν•¨μ
def get_callbacks(model_name):
    """λ¨λΈλ³„ μ½λ°± μ„¤μ •"""
    
    callbacks = [
        ModelCheckpoint(
            f'{model_name}_best.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_accuracy',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    return callbacks

# π† λ¨λΈ ν›λ ¨ ν•¨μ
def train_compatible_model(model_func, model_name):
    """νΈν™μ„± κ°μ„ λ λ¨λΈ ν›λ ¨"""
    print(f"\nπ§ {model_name} ν›λ ¨ μ¤‘...")
    
    try:
        # λ¨λΈ μƒμ„±
        with tf.device('/GPU:0' if gpus else '/CPU:0'):
            model = model_func()
        
        # λ°μ΄ν„° λ΅λ”©
        train_gen, val_gen = load_data(image_size=224, batch_size=16)
        
        # ν΄λμ¤ μ •λ³΄ μ¶λ ¥
        print(f"ν΄λμ¤ κ°μ: {train_gen.num_classes}")
        print(f"ν΄λμ¤ λΌλ²¨: {list(train_gen.class_indices.keys())}")
        
        # λ¨λΈ μ»΄νμΌ
        model = compile_model_compatible(model)
        
        # μ½λ°± μ„¤μ •
        callbacks = get_callbacks(model_name)
        
        # λ¨λΈ μ”μ•½
        print(f"\nπ“ {model_name} μ•„ν‚¤ν…μ²:")
        print(f"μ΄ νλΌλ―Έν„°: {model.count_params():,}")
        
        # ν›λ ¨ μ‹μ‘
        start_time = time.time()
        
        history = model.fit(
            train_gen,
            epochs=50,
            validation_data=val_gen,
            callbacks=callbacks,
            verbose=1
        )
        
        train_time = time.time() - start_time
        
        # κ²°κ³Ό κ³„μ‚°
        best_val_acc = max(history.history['val_accuracy'])
        final_val_acc = history.history['val_accuracy'][-1]
        final_train_acc = history.history['accuracy'][-1]
        overfitting = final_train_acc - final_val_acc
        
        print(f"β… {model_name} μ™„λ£!")
        print(f"   μµκ³  κ²€μ¦ μ •ν™•λ„: {best_val_acc:.4f}")
        print(f"   κ³Όμ ν•© μ μ: {overfitting:.4f}")
        print(f"   ν›λ ¨ μ‹κ°„: {train_time:.1f}μ΄")
        
        # 90% λ‹¬μ„± μ²΄ν¬
        if best_val_acc >= 0.9:
            print(f"π‰ {model_name}μ΄ 90% λ‹¬μ„±!")
        elif best_val_acc >= 0.85:
            print(f"π€ {model_name}μ΄ 85% λ‹¬μ„±!")
        
        return {
            'model_name': model_name,
            'best_val_acc': best_val_acc,
            'final_val_acc': final_val_acc,
            'final_train_acc': final_train_acc,
            'overfitting_score': overfitting,
            'train_time': train_time,
            'epochs': len(history.history['accuracy'])
        }, history
        
    except Exception as e:
        print(f"β {model_name} μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# π€ λ©”μΈ μ‹¤ν— μ‹¤ν–‰
print("π€ νΈν™μ„± κ°μ„ λ μ• λ‹λ©”μ΄μ… κ°μ • μΈμ‹ μ‹¤ν— μ‹μ‘!")

models_to_test = [
    (create_hybrid_attention_model, 'HybridAttention_EfficientNet'),
    (create_enhanced_efficientnet_model, 'Enhanced_EfficientNet'),
    (create_densenet_ensemble_model, 'DenseNet_Ensemble')
]

results = []
histories = {}

for model_func, model_name in models_to_test:
    result, history = train_compatible_model(model_func, model_name)
    
    if result:
        results.append(result)
        histories[model_name] = history

# π“ κ²°κ³Ό λ¶„μ„
print("\n" + "="*70)
print("π† νΈν™μ„± κ°μ„ λ μ• λ‹λ©”μ΄μ… κ°μ • μΈμ‹ κ²°κ³Ό")
print("="*70)

if results:
    # μ„±λ¥μ μ •λ ¬
    results.sort(key=lambda x: x['best_val_acc'], reverse=True)
    
    print(f"\nπ¥‡ μµμΆ… μμ„:")
    for i, result in enumerate(results, 1):
        status = "π‰ 90%+" if result['best_val_acc'] >= 0.9 else "π€ 85%+" if result['best_val_acc'] >= 0.85 else "β… 80%+" if result['best_val_acc'] >= 0.8 else "β οΈ"
        print(f"{i}. {status} {result['model_name']:<30} | {result['best_val_acc']:.4f} | κ³Όμ ν•©: {result['overfitting_score']:.4f}")
    
    # 90% λ‹¬μ„± λ¨λΈ
    success_models = [r for r in results if r['best_val_acc'] >= 0.9]
    
    if success_models:
        print(f"\nπ‰ 90% λ‹¬μ„± λ¨λΈλ“¤:")
        for model in success_models:
            print(f"  π† {model['model_name']}: {model['best_val_acc']:.4f}")
    else:
        best_model = results[0]
        print(f"\nπ― μµκ³  μ„±λ¥ λ¨λΈ:")
        print(f"  λ¨λΈ: {best_model['model_name']}")
        print(f"  μ •ν™•λ„: {best_model['best_val_acc']:.4f}")
        print(f"  90%κΉμ§€: {0.9 - best_model['best_val_acc']:.4f} λ¶€μ΅±")
    
    # μ‹κ°ν™”
    if histories:
        plt.figure(figsize=(15, 10))
        
        colors = ['blue', 'orange', 'green', 'red', 'purple']
        
        # μ •ν™•λ„ κ·Έλν”„
        plt.subplot(2, 2, 1)
        for i, (model_name, history) in enumerate(histories.items()):
            plt.plot(history.history['val_accuracy'], 
                    label=f'{model_name}', color=colors[i], linewidth=2)
        plt.axhline(y=0.9, color='red', linestyle='--', linewidth=2, label='90% λ©ν‘')
        plt.title('κ²€μ¦ μ •ν™•λ„ λΉ„κµ', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Validation Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # μ†μ‹¤ κ·Έλν”„
        plt.subplot(2, 2, 2)
        for i, (model_name, history) in enumerate(histories.items()):
            plt.plot(history.history['val_loss'], 
                    label=f'{model_name}', color=colors[i], linewidth=2)
        plt.title('κ²€μ¦ μ†μ‹¤ λΉ„κµ', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Validation Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # μµμΆ… μ„±λ¥ λ§‰λ€ κ·Έλν”„
        plt.subplot(2, 2, 3)
        model_names = [r['model_name'] for r in results]
        accuracies = [r['best_val_acc'] for r in results]
        bars = plt.bar(model_names, accuracies, color=['gold', 'silver', 'bronze'][:len(results)])
        plt.axhline(y=0.9, color='red', linestyle='--', linewidth=2, alpha=0.7)
        plt.title('μµκ³  μ„±λ¥ λΉ„κµ', fontsize=14, fontweight='bold')
        plt.ylabel('Best Validation Accuracy')
        plt.xticks(rotation=45)
        
        # κ°’ ν‘μ‹
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # κ³Όμ ν•© λ¶„μ„
        plt.subplot(2, 2, 4)
        overfitting_scores = [r['overfitting_score'] for r in results]
        plt.bar(model_names, overfitting_scores, color='lightcoral', alpha=0.7)
        plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='κ³Όμ ν•© κ²½κ³„')
        plt.title('κ³Όμ ν•© λ¶„μ„', fontsize=14, fontweight='bold')
        plt.ylabel('Overfitting Score')
        plt.xticks(rotation=45)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('compatible_anime_emotion_results.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # μµμΆ… κ²°λ΅ 
    max_accuracy = max(r['best_val_acc'] for r in results)
    
    print(f"\nπ― μµμΆ… κ²°κ³Ό:")
    print(f"μµκ³  λ‹¬μ„± μ •ν™•λ„: {max_accuracy:.4f}")
    
    if max_accuracy >= 0.9:
        print("π‰π‰π‰ 90% λ©ν‘ λ‹¬μ„±! λ€μ„±κ³µ!")
    elif max_accuracy >= 0.85:
        print("π― 85% μ΄μƒ λ‹¬μ„±! ν›λ¥­ν• κ²°κ³Ό!")
    elif max_accuracy >= 0.8:
        print("β… 80% μ΄μƒ λ‹¬μ„±! μΆ‹μ€ μ„±λ¥!")
    else:
        print("β οΈ μ¶”κ°€ μµμ ν™” ν•„μ”")

else:
    print("β λ¨λ“  μ‹¤ν—μ΄ μ‹¤ν¨ν–μµλ‹λ‹¤.")

print(f"\nπ νΈν™μ„± κ°μ„ λ μ• λ‹λ©”μ΄μ… κ°μ • μΈμ‹ μ‹¤ν— μ™„λ£!")

# π’΅ μ„±λ¥ ν–¥μƒ ν
print(f"\nπ’΅ μ„±λ¥ ν–¥μƒμ„ μ„ν• μ¶”κ°€ μ μ•:")
print("=" * 50)
print("1. π“ λ°μ΄ν„° μ¦κ°€:")
print("   - ν΄λμ¤λ³„ λ°μ΄ν„° λ°Έλ°μ‹±")
print("   - μ¨λΌμΈ λ°μ΄ν„° ν¬λ΅¤λ§")
print("   - κΈ°μ΅΄ μ΄λ―Έμ§€ νμ „/ν™•λ€ λ“±")

print("\n2. π”§ ν•μ΄νΌνλΌλ―Έν„° νλ‹:")
print("   - Learning Rate μ¤μΌ€μ¤„λ§")
print("   - Batch Size μµμ ν™”")
print("   - Dropout λΉ„μ¨ μ΅°μ •")

print("\n3. π― μ•™μƒλΈ” κΈ°λ²•:")
print("   - μ—¬λ¬ λ¨λΈ μ΅°ν•©")
print("   - Voting λλ” Averaging")
print("   - Stacking κΈ°λ²•")

print("\n4. π“ μ „μ΄ν•™μµ ν™μ©:")
print("   - λ” ν° λ°μ΄ν„°μ…‹μΌλ΅ pre-train")
print("   - Domain adaptation")
print("   - Fine-tuning μ „λµ κ°μ„ ")