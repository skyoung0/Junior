import torch
import sys
import platform

print("=== ì‹œìŠ¤í…œ ì •ë³´ ===")
print(f"Python ë²„ì „: {sys.version}")
print(f"í”Œë«í¼: {platform.platform()}")
print(f"ì•„í‚¤í…ì²˜: {platform.architecture()}")

print("\n=== PyTorch ì •ë³´ ===")
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print(f"CUDA ë²„ì „ (PyTorch): {torch.version.cuda}")
print(f"cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
print(f"cuDNN í™œì„±í™”: {torch.backends.cudnn.enabled}")

if torch.cuda.is_available():
    print("\n=== GPU ìƒì„¸ ì •ë³´ ===")
    print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {props.name}")
        print(f"  - ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
        print(f"  - ì»´í“¨íŠ¸ ëŠ¥ë ¥: {props.major}.{props.minor}")
        print(f"  - ë©€í‹°í”„ë¡œì„¸ì„œ: {props.multi_processor_count}")
    
    print(f"\ní˜„ì¬ GPU: {torch.cuda.current_device()}")
    print(f"GPU ì´ë¦„: {torch.cuda.get_device_name()}")
    
    print("\n=== GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ===")
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.3f} GB")
    print(f"ìºì‹œëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_reserved() / 1024**3:.3f} GB")
    
    print("\n=== GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ===")
    try:
        # í° í…ì„œë¡œ í…ŒìŠ¤íŠ¸
        device = torch.device('cuda:0')
        print(f"ë””ë°”ì´ìŠ¤: {device}")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        torch.cuda.empty_cache()
        
        x = torch.randn(5000, 5000, device=device)
        y = torch.randn(5000, 5000, device=device)
        
        print(f"í…ì„œ ìƒì„± í›„ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.3f} GB")
        
        # í–‰ë ¬ ê³±ì…ˆ
        import time
        start_time = time.time()
        z = torch.mm(x, y)
        end_time = time.time()
        
        print(f"âœ… í–‰ë ¬ ê³±ì…ˆ ì„±ê³µ!")
        print(f"   - ì—°ì‚° ì‹œê°„: {end_time - start_time:.3f}ì´ˆ")
        print(f"   - ê²°ê³¼ í¬ê¸°: {z.shape}")
        print(f"   - ê²°ê³¼ ë””ë°”ì´ìŠ¤: {z.device}")
        print(f"ì—°ì‚° í›„ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.3f} GB")
        
        # ì •ë¦¬
        del x, y, z
        torch.cuda.empty_cache()
        print("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ GPU ì—°ì‚° ì‹¤íŒ¨: {e}")
        
else:
    print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥")
    print("\nê°€ëŠ¥í•œ ì›ì¸:")
    print("- NVIDIA ë“œë¼ì´ë²„ ë¬¸ì œ")
    print("- CUDA ì„¤ì¹˜ ë¬¸ì œ") 
    print("- PyTorch CUDA ë²„ì „ ë¶ˆì¼ì¹˜")
    print("- í™˜ê²½ ë³€ìˆ˜ ë¬¸ì œ")

print("\nğŸ‰ ì™„ì „í•œ GPU í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")