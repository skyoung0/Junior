import numpy as np
import pandas as pd
import os

# GPU ì„¤ì •
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì„¤ì • ì™„ë£Œ: RTX 3060 ì‚¬ìš©")
    except RuntimeError as e:
        print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# ê²½ë¡œ ì„¤ì •
os.chdir('C:/Users/ayaan/Documents/Git/Junior/Python/Data')
train_dir = 'ani/train'
val_dir = 'ani/val'

# ğŸ¯ ê³¼ì†Œì í•© ìƒíƒœì—ì„œ ì•„ì£¼ ì¡°ê¸ˆë§Œ ì™„í™”
data_generator_with_aug = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    horizontal_flip=True,
    rotation_range=15,         # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ (20â†’15)
    width_shift_range=0.15,    # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ (0.2â†’0.15)
    height_shift_range=0.15,   # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ (0.2â†’0.15)
    zoom_range=0.15,          # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ (0.2â†’0.15)
    shear_range=0.05,         # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ (0.1â†’0.05)
    brightness_range=[0.9, 1.1],  # ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì‚´ì§ë§Œ ê°ì†Œ
    fill_mode='nearest'
)

data_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)

image_size = 224
batch_size = 32

train_generator = data_generator_with_aug.flow_from_directory(
    directory=train_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

validation_generator = data_generator_no_aug.flow_from_directory(
    directory=val_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

num_classes = train_generator.num_classes
print(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
print(f"í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {train_generator.samples}")
print(f"ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {validation_generator.samples}")

# ğŸ¯ ê³¼ì†Œì í•©ì—ì„œ ì•„ì£¼ ì¡°ê¸ˆë§Œ ì™„í™”ëœ ëª¨ë¸
with tf.device('/GPU:0' if gpus else '/CPU:0'):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    
    # ğŸ”“ ê³¼ì†Œì í•© ë•Œë³´ë‹¤ ì•„ì£¼ ì¡°ê¸ˆë§Œ ë” ë§ì€ ë ˆì´ì–´ í›ˆë ¨
    for layer in base_model.layers[:-15]:  # ê³¼ì†Œì í•© ë•Œ 10ê°œ â†’ 15ê°œ (ì¡°ê¸ˆë§Œ ì¦ê°€)
        layer.trainable = False
    
    cnn = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        
        # ğŸ¯ ì •ê·œí™” ì•„ì£¼ ì¡°ê¸ˆë§Œ ì™„í™”
        Dropout(0.6),                    # ê³¼ì†Œì í•© ë•Œ 0.7 â†’ 0.6 (ì¡°ê¸ˆë§Œ ê°ì†Œ)
        Dense(384,                       # ê³¼ì†Œì í•© ë•Œ 256 â†’ 384 (ì¡°ê¸ˆë§Œ ì¦ê°€)
              activation='relu', 
              kernel_regularizer=l2(0.008)),  # ê³¼ì†Œì í•© ë•Œ 0.01 â†’ 0.008 (ì¡°ê¸ˆë§Œ ê°ì†Œ)
        BatchNormalization(),
        Dropout(0.4),                    # ê³¼ì†Œì í•© ë•Œ 0.5 â†’ 0.4 (ì¡°ê¸ˆë§Œ ê°ì†Œ)
        
        Dense(num_classes, activation='softmax')
    ])

# ğŸ¯ í•™ìŠµë¥ ë„ ì•„ì£¼ ì¡°ê¸ˆë§Œ ì¦ê°€
cnn.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=0.00007),  # ê³¼ì†Œì í•© ë•Œ 0.00005 â†’ 0.00007 (ì¡°ê¸ˆë§Œ ì¦ê°€)
    metrics=['accuracy']
)

print("ì„¸ë°€í•˜ê²Œ ì¡°ì •ëœ ëª¨ë¸ êµ¬ì¡°:")
cnn.summary()

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('model', exist_ok=True)

# ğŸ¯ ì½œë°±ë„ ì¡°ê¸ˆ ëœ ì—„ê²©í•˜ê²Œ
callbacks_list = [
    ModelCheckpoint(
        "model/fine_tuned_balanced_model.h5",
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        mode='max'
    ),
    # Early Stoppingì„ ì¡°ê¸ˆ ëœ ì—„ê²©í•˜ê²Œ
    EarlyStopping(
        monitor='val_accuracy',
        patience=7,          # ê³¼ì†Œì í•© ë•Œ 5 â†’ 7 (ì¡°ê¸ˆë§Œ ì¦ê°€)
        verbose=1,
        restore_best_weights=True,
        min_delta=0.001
    ),
    # í•™ìŠµë¥  ê°ì†Œë„ ì¡°ê¸ˆ ëœ ì ê·¹ì ìœ¼ë¡œ
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.6,          # ê³¼ì†Œì í•© ë•Œ 0.5 â†’ 0.6 (ëœ ê¸‰ê²©í•˜ê²Œ)
        patience=4,
        min_lr=1e-7,
        verbose=1
    )
]

print(f"ğŸ¯ ì„¸ë°€í•œ ê· í˜• ì¡°ì • í›ˆë ¨ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}")

with tf.device('/GPU:0' if gpus else '/CPU:0'):
    hist = cnn.fit(
        train_generator,
        epochs=35,           # ì ë‹¹í•œ ì—í¬í¬ ìˆ˜
        validation_data=validation_generator,
        callbacks=callbacks_list,
        verbose=1
    )

print("ğŸ‰ ì„¸ë°€í•œ ì¡°ì • ì™„ë£Œ!")

# ğŸ“Š ì´ì „ ê²°ê³¼ë“¤ê³¼ ë¹„êµí•˜ëŠ” ì‹œê°í™”
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8-whitegrid')
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

epochs_range = range(1, len(hist.history['accuracy']) + 1)

# 1. ì •í™•ë„ ê·¸ë˜í”„
ax1.plot(epochs_range, hist.history['accuracy'], 'b-', linewidth=3, label='Training Accuracy', marker='o', markersize=4)
ax1.plot(epochs_range, hist.history['val_accuracy'], 'r-', linewidth=3, label='Validation Accuracy', marker='s', markersize=4)

# ëª©í‘œ êµ¬ê°„ í‘œì‹œ
ax1.axhspan(0.6, 0.8, alpha=0.2, color='green', label='ëª©í‘œ ê²€ì¦ ì •í™•ë„ êµ¬ê°„')
ax1.axhspan(0.7, 0.9, alpha=0.1, color='blue', label='ëª©í‘œ í›ˆë ¨ ì •í™•ë„ êµ¬ê°„')

ax1.set_title('Fine-Tuned Balance Model', fontsize=16, fontweight='bold', color='darkblue')
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Accuracy', fontsize=12)
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, 1.0)

# 2. ì†ì‹¤ ê·¸ë˜í”„
ax2.plot(epochs_range, hist.history['loss'], 'b-', linewidth=3, label='Training Loss', marker='o', markersize=4)
ax2.plot(epochs_range, hist.history['val_loss'], 'r-', linewidth=3, label='Validation Loss', marker='s', markersize=4)
ax2.set_title('Loss Progression', fontsize=16, fontweight='bold', color='darkgreen')
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Loss', fontsize=12)
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)

# 3. ê³¼ì í•© ë¶„ì„ (ì„¸ë°€í•˜ê²Œ)
train_acc = np.array(hist.history['accuracy'])
val_acc = np.array(hist.history['val_accuracy'])
gap = train_acc - val_acc

ax3.plot(epochs_range, gap, 'purple', linewidth=3, marker='d', markersize=4)
ax3.axhline(y=0, color='black', linestyle='-', alpha=0.8, linewidth=2, label='ì™„ë²½í•œ ê· í˜•')
ax3.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='í—ˆìš© ê°€ëŠ¥í•œ ê³¼ì í•© (+10%)')
ax3.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='ê³¼ì í•© ê²½ê³  (+20%)')
ax3.axhline(y=-0.05, color='blue', linestyle='--', alpha=0.7, label='ê³¼ì†Œì í•© ê²½ê³„ (-5%)')

# ìµœì  êµ¬ê°„ ê°•ì¡°
ax3.fill_between(epochs_range, -0.05, 0.1, alpha=0.3, color='lightgreen', label='ìµœì  êµ¬ê°„')

ax3.set_title('Overfitting Analysis (Fine-Tuned)', fontsize=16, fontweight='bold', color='purple')
ax3.set_xlabel('Epoch', fontsize=12)
ax3.set_ylabel('Accuracy Gap (Train - Val)', fontsize=12)
ax3.legend(fontsize=10)
ax3.grid(True, alpha=0.3)

# 4. ì´ì „ ì‹œë„ë“¤ê³¼ì˜ ë¹„êµ
ax4.clear()
attempts = ['1st\n(Overfit)', '2nd\n(Underfit)', '3rd\n(Overfit Again)', '4th\n(Fine-Tuned)']
train_accs = [0.95, 0.18, 0.95, hist.history['accuracy'][-1]]
val_accs = [0.65, 0.34, 0.65, hist.history['val_accuracy'][-1]]

x = np.arange(len(attempts))
width = 0.35

bars1 = ax4.bar(x - width/2, train_accs, width, label='Training Acc', color='lightblue', alpha=0.8)
bars2 = ax4.bar(x + width/2, val_accs, width, label='Validation Acc', color='lightcoral', alpha=0.8)

# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
for i, (train, val) in enumerate(zip(train_accs, val_accs)):
    ax4.text(i - width/2, train + 0.02, f'{train:.2f}', ha='center', va='bottom', fontweight='bold')
    ax4.text(i + width/2, val + 0.02, f'{val:.2f}', ha='center', va='bottom', fontweight='bold')

ax4.set_title('Progress Comparison', fontsize=16, fontweight='bold', color='darkred')
ax4.set_ylabel('Accuracy', fontsize=12)
ax4.set_xticks(x)
ax4.set_xticklabels(attempts)
ax4.legend(fontsize=11)
ax4.grid(True, alpha=0.3, axis='y')
ax4.set_ylim(0, 1.0)

plt.tight_layout()
plt.savefig('model/fine_tuned_progress.png', dpi=300, bbox_inches='tight')
plt.show()

# ğŸ¯ ì„¸ë°€í•œ ë¶„ì„
final_train_acc = hist.history['accuracy'][-1]
final_val_acc = hist.history['val_accuracy'][-1]
best_val_acc = max(hist.history['val_accuracy'])
best_epoch = hist.history['val_accuracy'].index(best_val_acc) + 1
final_gap = final_train_acc - final_val_acc

print(f"\nğŸ¯ === ì„¸ë°€í•œ ì¡°ì • ê²°ê³¼ ===")
print(f"ìµœì¢… í›ˆë ¨ ì •í™•ë„: {final_train_acc:.4f}")
print(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}")
print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f} (ì—í¬í¬ {best_epoch})")
print(f"ğŸ“Š ìµœì¢… ê²©ì°¨: {final_gap:.4f}")

print(f"\nğŸ“ˆ === ì§„í–‰ ìƒí™© ë¹„êµ ===")
print(f"1ì°¨ ì‹œë„ (ê³¼ì í•©):     í›ˆë ¨ 95.0% vs ê²€ì¦ 65.0% (ê²©ì°¨: 30.0%)")
print(f"2ì°¨ ì‹œë„ (ê³¼ì†Œì í•©):   í›ˆë ¨ 18.0% vs ê²€ì¦ 34.0% (ê²©ì°¨: -16.0%)")
print(f"3ì°¨ ì‹œë„ (ë‹¤ì‹œê³¼ì í•©): í›ˆë ¨ 95.0% vs ê²€ì¦ 65.0% (ê²©ì°¨: 30.0%)")
print(f"4ì°¨ ì‹œë„ (ì„¸ë°€ì¡°ì •):   í›ˆë ¨ {final_train_acc*100:.1f}% vs ê²€ì¦ {final_val_acc*100:.1f}% (ê²©ì°¨: {final_gap*100:.1f}%)")

# ğŸ¯ ê°œì„ ë„ í‰ê°€
if 0.05 <= final_gap <= 0.15:
    print("ğŸ‰ ë“œë””ì–´ ì ì ˆí•œ ê· í˜•ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
    status = "BALANCED"
elif final_gap > 0.2:
    print("ğŸ˜… ì•„ì§ë„ ê³¼ì í•©ì…ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ë” ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤.")
    status = "STILL_OVERFIT"
elif final_gap < -0.05:
    print("ğŸ”µ ì•„ì§ ê³¼ì†Œì í•©ì…ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ì¡°ê¸ˆ ë” ì™„í™”í•´ì•¼ í•©ë‹ˆë‹¤.")
    status = "STILL_UNDERFIT"
else:
    print("âœ… ê· í˜•ì— ê·¼ì ‘í–ˆìŠµë‹ˆë‹¤!")
    status = "NEAR_BALANCED"

print(f"\nğŸ” === ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ ===")
if status == "BALANCED":
    print("ğŸ¯ ì™„ë²½! ì´ì œ ë°ì´í„°ë¥¼ ë” ìˆ˜ì§‘í•˜ê±°ë‚˜ ì•™ìƒë¸” ê¸°ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
elif status == "STILL_OVERFIT":
    print("ğŸ”§ Dropoutì„ 0.1 ë” ëŠ˜ë¦¬ê³ , L2 ì •ê·œí™”ë¥¼ 0.002 ë” ëŠ˜ë ¤ë³´ì„¸ìš”.")
elif status == "STILL_UNDERFIT":
    print("ğŸ”§ Dropoutì„ 0.05 ì¤„ì´ê³ , í›ˆë ¨ ê°€ëŠ¥í•œ ë ˆì´ì–´ë¥¼ 5ê°œ ë” ëŠ˜ë ¤ë³´ì„¸ìš”.")
else:
    print("ğŸ¯ í•œ ë²ˆ ë” ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ì™„ë²½í•œ ê· í˜•ì„ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤!")

print(f"\nğŸ’¾ ì €ì¥ëœ íŒŒì¼: model/fine_tuned_balanced_model.h5")
print(f"ğŸ’¾ ì§„í–‰ìƒí™© ê·¸ë˜í”„: model/fine_tuned_progress.png")