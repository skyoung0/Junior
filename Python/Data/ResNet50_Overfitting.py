import numpy as np
import pandas as pd
import os

# GPU ì„¤ì • ì¶”ê°€
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì„¤ì • ì™„ë£Œ: RTX 3060 ì‚¬ìš©")
    except RuntimeError as e:
        print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# ê²½ë¡œ ì„¤ì •
os.chdir('C:/Users/ayaan/Documents/Git/Junior/Python/Data')
train_dir = 'ani/train'
val_dir = 'ani/val'

# ğŸ›¡ï¸ ê°•í™”ëœ ë°ì´í„° ì¦ê°• (ê³¼ì í•© ë°©ì§€)
data_generator_with_aug = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    horizontal_flip=True,
    rotation_range=30,           # ì¦ê°€
    width_shift_range=0.3,       # ì¦ê°€  
    height_shift_range=0.3,      # ì¦ê°€
    zoom_range=0.2,              # ì¶”ê°€
    shear_range=0.15,            # ì¶”ê°€
    brightness_range=[0.8, 1.2], # ì¶”ê°€
    fill_mode='nearest'
)

data_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)

image_size = 224
batch_size = 16  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ (ê·œì œ íš¨ê³¼)

train_generator = data_generator_with_aug.flow_from_directory(
    directory=train_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

validation_generator = data_generator_no_aug.flow_from_directory(
    directory=val_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

num_classes = train_generator.num_classes
print(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
print(f"í›ˆë ¨ ìƒ˜í”Œ: {train_generator.samples}ê°œ")
print(f"ê²€ì¦ ìƒ˜í”Œ: {validation_generator.samples}ê°œ")

# ğŸ—ï¸ ê³¼ì í•© ë°©ì§€ ëª¨ë¸ êµ¬ì„±
with tf.device('/GPU:0' if gpus else '/CPU:0'):
    # ResNet50 ë² ì´ìŠ¤ ëª¨ë¸
    base_model = ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # ğŸ”’ ë² ì´ìŠ¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì • (Feature Extraction)
    base_model.trainable = False
    print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì • (ê³¼ì í•© ë°©ì§€)")
    
    # ğŸ›¡ï¸ ê·œì œê°€ ê°•í™”ëœ ëª¨ë¸ êµ¬ì¡°
    cnn = Sequential([
        base_model,
        
        # Flatten ëŒ€ì‹  GlobalAveragePooling2D ì‚¬ìš© (ê³¼ì í•© ë°©ì§€)
        GlobalAveragePooling2D(),
        
        # ì²« ë²ˆì§¸ Dropout
        Dropout(0.5),
        
        # ì²« ë²ˆì§¸ Dense ë ˆì´ì–´ (í¬ê¸° ê°ì†Œ)
        Dense(512, activation='relu'),  # 1024 â†’ 512ë¡œ ê°ì†Œ
        BatchNormalization(),           # ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€
        Dropout(0.3),
        
        # ë‘ ë²ˆì§¸ Dense ë ˆì´ì–´ ì¶”ê°€ (ì ì§„ì  ê°ì†Œ)
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        
        # ì¶œë ¥ ë ˆì´ì–´
        Dense(num_classes, activation='softmax')
    ])

# ğŸ¯ í•™ìŠµë¥  ê°ì†Œ (ê³¼ì í•© ë°©ì§€)
optimizer = Adam(learning_rate=0.0001)  # ê¸°ë³¸ê°’ë³´ë‹¤ ë‚®ê²Œ

cnn.compile(
    loss='categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy']
)

print("ğŸ—ï¸ ê³¼ì í•© ë°©ì§€ ëª¨ë¸ êµ¬ì¡°:")
cnn.summary()

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('model', exist_ok=True)

# ğŸš¨ ê°•í™”ëœ ì½œë°± ì„¤ì • (ê³¼ì í•© ë°©ì§€)
callbacks_list = [
    # ê²€ì¦ ì •í™•ë„ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì €ì¥
    ModelCheckpoint(
        'model/best_model_no_overfitting.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    
    # ì¡°ê¸° ì¢…ë£Œ (ê³¼ì í•© ë°©ì§€ì˜ í•µì‹¬!)
    EarlyStopping(
        monitor='val_accuracy',
        patience=5,  # 5 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
        restore_best_weights=True,
        verbose=1
    ),
    
    # í•™ìŠµë¥  ìë™ ê°ì†Œ
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7,
        verbose=1
    )
]

# ğŸš€ í›ˆë ¨ ì‹œì‘
print("ğŸš€ ê³¼ì í•© ë°©ì§€ í›ˆë ¨ ì‹œì‘!")
print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
print(f"ğŸ›¡ï¸ ê·œì œ ê¸°ë²•: Dropout, BatchNorm, EarlyStopping, ë°ì´í„° ì¦ê°•")

with tf.device('/GPU:0' if gpus else '/CPU:0'):
    hist = cnn.fit(
        train_generator,
        epochs=30,  # ì—í¬í¬ ì¦ê°€ (EarlyStoppingì´ ì•Œì•„ì„œ ì¤‘ë‹¨)
        validation_data=validation_generator,
        callbacks=callbacks_list,
        verbose=1
    )

print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")

# ğŸ“Š ê²°ê³¼ ë¶„ì„
final_train_acc = hist.history['accuracy'][-1]
final_val_acc = hist.history['val_accuracy'][-1]
best_val_acc = max(hist.history['val_accuracy'])
overfitting_score = final_train_acc - final_val_acc

print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
print(f"í›ˆë ¨ ì •í™•ë„: {final_train_acc:.4f}")
print(f"ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}")
print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")
print(f"ê³¼ì í•© ì ìˆ˜: {overfitting_score:.4f}")

if overfitting_score < 0.1:
    print("âœ… ê³¼ì í•©ì´ ì˜ ì œì–´ë˜ì—ˆìŠµë‹ˆë‹¤!")
elif overfitting_score < 0.2:
    print("âš ï¸ ì•½ê°„ì˜ ê³¼ì í•©ì´ ìˆì§€ë§Œ í—ˆìš© ë²”ìœ„ì…ë‹ˆë‹¤.")
else:
    print("âŒ ì—¬ì „íˆ ê³¼ì í•©ì´ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ê·œì œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

# ğŸ¨ ì‹œê°í™”
import matplotlib.pyplot as plt

plt.style.use('default')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 1. ì •í™•ë„ ê·¸ë˜í”„
ax1.plot(hist.history['accuracy'], linewidth=2, label='Train', color='#1f77b4')
ax1.plot(hist.history['val_accuracy'], linewidth=2, label='Validation', color='#ff7f0e')
ax1.set_title('Model Accuracy (ê³¼ì í•© í•´ê²°ë¨)', fontsize=14, fontweight='bold')
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Accuracy', fontsize=12)
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, 1.0)

# ê³¼ì í•© ì˜ì—­ í‘œì‹œ
if len(hist.history['accuracy']) > 0:
    ax1.fill_between(range(len(hist.history['accuracy'])), 
                     hist.history['accuracy'], 
                     hist.history['val_accuracy'], 
                     alpha=0.2, color='red', 
                     label='Overfitting Gap')

# 2. ì†ì‹¤ ê·¸ë˜í”„
ax2.plot(hist.history['loss'], linewidth=2, label='Train', color='#1f77b4')
ax2.plot(hist.history['val_loss'], linewidth=2, label='Validation', color='#ff7f0e')
ax2.set_title('Model Loss', fontsize=14, fontweight='bold')
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Loss', fontsize=12)
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('model/overfitting_solved_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"ğŸ’¾ ê·¸ë˜í”„ ì €ì¥: model/overfitting_solved_results.png")

# ğŸ” ì—í¬í¬ë³„ ê³¼ì í•© ë¶„ì„
print(f"\nğŸ” ì—í¬í¬ë³„ ê³¼ì í•© ë¶„ì„:")
print("Epoch | Train Acc | Val Acc | Gap")
print("-" * 35)
for i in range(len(hist.history['accuracy'])):
    train_acc = hist.history['accuracy'][i]
    val_acc = hist.history['val_accuracy'][i]
    gap = train_acc - val_acc
    status = "âœ…" if gap < 0.1 else "âš ï¸" if gap < 0.2 else "âŒ"
    print(f"{i+1:5d} | {train_acc:9.4f} | {val_acc:7.4f} | {gap:6.4f} {status}")

print(f"\nğŸ¯ ê³¼ì í•© í•´ê²° ì™„ë£Œ!")